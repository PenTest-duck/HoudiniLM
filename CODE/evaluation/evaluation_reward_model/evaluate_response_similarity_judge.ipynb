{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65a1c4c2",
   "metadata": {},
   "source": [
    "# Evaluate response similarity scoring logic\n",
    "This script uses a custom labelled dataset to evaluate the performance of the prompt-response similarity scoring (which uses LLM as a judge).\n",
    "This measures how well our judge LLM is able to determine the intent similarity between a prompt and a response, compared against human labelled ground truths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a58d81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(\"../../\")\n",
    "sys.path.append(project_root)\n",
    "from reward_model.strongreject_score import calculate_strongreject_rubric_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60837b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{project_root}/../MISC/datasets/custom/response_similarity_eval_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc5bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/99 [00:00<?, ?it/s]/var/folders/xt/72wpw_1j665gjkbv6nhyvk5h0000gn/T/ipykernel_53440/255777674.py:31: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = results_df._append(results, ignore_index=True)\n",
      "  8%|â–Š         | 8/99 [00:16<03:15,  2.15s/it]"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "ground_truth_scores = []\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\n",
    "    'prompt', 'response', 'predicted_score', 'ground_truth'\n",
    "])\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    prompt = row['prompt']\n",
    "    response = row['response']\n",
    "    \n",
    "    # Calculate reward using our function\n",
    "    while True:\n",
    "        try:\n",
    "            reward_score = calculate_strongreject_rubric_score(prompt, response)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error in StrongREJECT: {e}\")\n",
    "    scores.append(reward_score)\n",
    "    \n",
    "    # Calculate ground truth (average of score1 and score2)\n",
    "    ground_truth = row[\"avg_normalised\"]\n",
    "    ground_truth_scores.append(ground_truth)\n",
    "\n",
    "    results = {\n",
    "        'prompt': prompt,\n",
    "        'response': response,\n",
    "        'predicted_score': reward_score,\n",
    "        'ground_truth': ground_truth\n",
    "    }\n",
    "    results_df = results_df._append(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefe8e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = results_df['predicted_score'].corr(results_df['ground_truth'])\n",
    "mae = abs(results_df['predicted_score'] - results_df['ground_truth']).mean()\n",
    "print(f\"Correlation: {correlation:.4f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
